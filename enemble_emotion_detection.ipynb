{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a189af9b-50e6-4be8-9c03-16e2408185a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import img_to_array,  load_img\n",
    "\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model,Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee3666-671a-4265-af77-d63c98b8e37e",
   "metadata": {},
   "source": [
    "## Second model\n",
    "Using a pre-trained model ontop of our own"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09e975-cde7-47a4-97cc-0f195b764c67",
   "metadata": {},
   "source": [
    "## 1. Choose a Pretrained Model:\n",
    "Selection: Use VGG16 from tensorflow.keras.applications for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678d014e-5cbc-45a1-871e-23ddb36e9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "model_VGG16 = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897aa4c-88af-462f-8211-5f6f28e28c4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocess the Images:\n",
    "Resize and Convert to Tensor:\n",
    "Resize the images to the required input shape (32x32x3) and convert them to tensors. Convert Grayscale to RG - Resize and Normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631c7989-5644-4176-a4c4-55c5a12dded0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23060 images belonging to 7 classes.\n",
      "Found 5761 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# data generator that will load and preprocess your grayscale images, convert them to RGB, \n",
    "# resize them to the required input shape (32x32x3), and normalize the pixel values to the range [0, 1]. \n",
    "# This data can be fed to the VGG16 model for transfer learning.\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2,  # Split for validation\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "# Load the images and convert to tensors\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'content/train',\n",
    "    target_size=(32, 32),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',  # Convert grayscale to RGB\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'content/train',\n",
    "    target_size=(32, 32),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',  # Convert grayscale to RGB\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34768fe6-a495-41a2-a98e-91c2d384d6ce",
   "metadata": {},
   "source": [
    "## 3. Setup the Transfer Learning Model:\n",
    "Extract Features with Pretrained Base Model:\n",
    "Remove the top layers of the VGG16 model and use only the bottom layers.\n",
    "Freeze the weights of the pretrained layers to prevent further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d702de30-60ae-4bf6-a001-6478f63b8a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)           1792      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)           36928     ['block1_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)           0         ['block1_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)          73856     ['block1_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)          147584    ['block2_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)            0         ['block2_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)            295168    ['block2_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)            590080    ['block3_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)            590080    ['block3_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)            0         ['block3_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)            1180160   ['block3_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)            2359808   ['block4_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)            2359808   ['block4_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)            0         ['block4_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)            2359808   ['block4_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)            2359808   ['block5_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)            2359808   ['block5_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)            0         ['block5_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 48, 48, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 512)                  0         ['block5_pool[0][0]']         \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 7)                    1328167   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 519)                  0         ['flatten[0][0]',             \n",
      "                                                                     'sequential_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  66560     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 7)                    903       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16110318 (61.46 MB)\n",
      "Trainable params: 1393454 (5.32 MB)\n",
      "Non-trainable params: 14716864 (56.14 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Flatten, Concatenate\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your emotion classification model\n",
    "emotion_model = load_model('model.h5', compile=False)\n",
    "\n",
    "# Modify the emotion model to accept the VGG16 output shape (48, 48, 1)\n",
    "emotion_model_input = Input(shape=(48, 48, 1))\n",
    "emotion_model_output = emotion_model(emotion_model_input)\n",
    "\n",
    "# Flatten the VGG16 output\n",
    "vgg16_output = Flatten()(model_VGG16.output)\n",
    "\n",
    "# Concatenate the outputs\n",
    "combined_output = Concatenate()([vgg16_output, emotion_model_output])\n",
    "\n",
    "# Add more layers as needed for your specific task\n",
    "# For example:\n",
    "x = Dense(128, activation='relu')(combined_output)\n",
    "output = Dense(7, activation='softmax')(x)  # Assuming 7 classes for your emotion model\n",
    "\n",
    "# Create the combined model\n",
    "combined_model = Model(inputs=[model_VGG16.input, emotion_model_input], outputs=output)\n",
    "\n",
    "# Freeze the base VGG16 layers\n",
    "for layer in model_VGG16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model\n",
    "combined_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddf144-9d71-42ff-9c18-865b3b39ebb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Limit the Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6019e58-b33f-46a2-adbe-3d8d56c7132c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine batches from the limited generators\n",
    "from itertools import islice\n",
    "from skimage.transform import resize\n",
    "\n",
    "def custom_train_generator(generator, emotion_model):\n",
    "    target_size = (48, 48, 1)  # Set the desired target size for emotion model\n",
    "    while True:\n",
    "        for batch_data, batch_labels in generator:\n",
    "            # Resize the data to match emotion model's input shape\n",
    "            resized_batch_data = np.array([resize(img, target_size) for img in batch_data])\n",
    "            emotion_predictions = emotion_model.predict(resized_batch_data)\n",
    "            yield [resized_batch_data, emotion_predictions], batch_labels\n",
    "\n",
    "def custom_validation_generator(generator, emotion_model):\n",
    "    target_size = (48, 48, 1)  # Set the desired target size for emotion model\n",
    "    while True:\n",
    "        for batch_data, batch_labels in generator:\n",
    "            # Resize the data to match emotion model's input shape\n",
    "            resized_batch_data = np.array([resize(img, target_size) for img in batch_data])\n",
    "            emotion_predictions = emotion_model.predict(resized_batch_data)\n",
    "            yield [resized_batch_data, emotion_predictions], batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ff296a-85f1-49ef-b5a4-2986f99fe309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def limit_generator(generator, limit):\n",
    "    for i, (data, labels) in enumerate(generator):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        yield data, labels\n",
    "\n",
    "# Define the desired dataset size\n",
    "desired_dataset_size = 500\n",
    "\n",
    "# Calculate the number of batches to take based on the available batches\n",
    "train_batches_to_take = min(desired_dataset_size // train_generator.batch_size, len(train_generator))\n",
    "validation_batches_to_take = min(desired_dataset_size // validation_generator.batch_size, len(validation_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f10f3-f4e6-4a6c-b258-5e80dea49806",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed809fbc-ac29-4638-9ce9-b3ce697ed9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the training generator: 721\n",
      "Number of batches in the validation generator: 181\n",
      "Number of samples per batch in the training generator: 32\n",
      "Number of samples per batch in the validation generator: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches in the training generator:\", len(train_generator))\n",
    "print(\"Number of batches in the validation generator:\", len(validation_generator))\n",
    "print(\"Number of samples per batch in the training generator:\", train_generator.batch_size)\n",
    "print(\"Number of samples per batch in the validation generator:\", validation_generator.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "607ddbca-adb4-4b69-b29f-79d72fbc1df9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Data shape: (32, 32, 32, 3)\n",
      "Label shape: (32, 7)\n",
      "Batch 1\n",
      "Data shape: (32, 32, 32, 3)\n",
      "Label shape: (32, 7)\n"
     ]
    }
   ],
   "source": [
    "for i, (data_batch, label_batch) in enumerate(train_generator):\n",
    "    print(\"Batch\", i)\n",
    "    print(\"Data shape:\", data_batch.shape)\n",
    "    print(\"Label shape:\", label_batch.shape)\n",
    "    if i == 1:  # Print the first 5 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14744b3-c262-4f75-8a74-5f1529b81276",
   "metadata": {},
   "source": [
    "### Custom Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "644bbbd0-7f20-46cf-9839-a1f9a2adeeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset the generators\n",
    "limited_train_generator = islice(train_generator, train_batches_to_take)\n",
    "limited_validation_generator = islice(validation_generator, validation_batches_to_take)\n",
    "\n",
    "# Create modified generators\n",
    "limited_train_generator = custom_train_generator(limited_train_generator, emotion_model)\n",
    "limited_validation_generator = custom_validation_generator(limited_validation_generator, emotion_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5131853-7768-49ac-9347-ea601471d21c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b7935e5-5e0d-4d82-8cc6-052e4eab6292",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(32, 48, 48, 1) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m limited_validation_generator \u001b[38;5;241m=\u001b[39m custom_validation_generator(limited_validation_generator, combined_model)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimited_train_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimited_validation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mcustom_train_generator\u001b[1;34m(generator, emotion_model)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data, batch_labels \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Resize the data to match emotion model's input shape\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     resized_batch_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([resize(img, target_size) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m batch_data])\n\u001b[1;32m---> 11\u001b[0m     emotion_predictions \u001b[38;5;241m=\u001b[39m \u001b[43memotion_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_batch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m [resized_batch_data, emotion_predictions], batch_labels\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filere97twm2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\asche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(32, 48, 48, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Reset the generators\n",
    "limited_train_generator = islice(train_generator, train_batches_to_take)\n",
    "limited_validation_generator = islice(validation_generator, validation_batches_to_take)\n",
    "\n",
    "# Create modified generators\n",
    "limited_train_generator = custom_train_generator(limited_train_generator, combined_model)\n",
    "limited_validation_generator = custom_validation_generator(limited_validation_generator, combined_model)\n",
    "\n",
    "# Train the model\n",
    "history = combined_model.fit(\n",
    "    limited_train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=limited_validation_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba707c-3323-4be0-a0af-27aa5f8b25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "loss, accuracy = combined_model.evaluate(limited_validation_generator)\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
